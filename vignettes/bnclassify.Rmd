---
title: "The `bnclassify` package"
author: "Bojan Mihaljevic, Concha Bielza, Pedro Larranaga"
date: "`r Sys.Date()`"
output: 
  rmarkdown::pdf_document:
    toc: true
    number_sections: true
fontsize: 11pt
vignette: >
  %\VignetteIndexEntry{bnclassify}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

# Introduction
The `bnclassify` package implements algorithms for learning discrete Bayesian 
network classifiers from data. It handles both incomplete and complete data, although it is much better suited for the latter. Prediction with incomplete data is notably slower, rendering the wrapper learning algorithms infeasible in some cases, whereas parameter estimation is no longer that of maximum likelihood. 

We begin with an example showing the main functionalities and then go into some detail with structure and parameter learning, prediction, cross-validation, and how to leverage related `R` packages.

# An example
This sections shows some of the main functionalities. 

First, we load the package and an included data set, `car`. 
```{r}
library(bnclassify)
data(car)
summary(car)
```

Now, we the learn a naive Bayes from the `car` data set. 
```{r}
a <- nb('class', car)
a
```

`nb` has returned a `bnc_dag` object, which contains just the network structure, without any parameters. 

We can query this object for its features, it factorization type (e.g., whether is a naive Bayes), or plot its network structure.
```{r}
features(a)
is_nb(a)
```

```{r}
plot(a)
```
For more functions to query a `bnc_dag` object, see `?bnc_dag_object`.

We need to learn the parameters before we can classify unseen data. We do this with the `lp` function.

```{r}
b <- lp(a, car, smooth = 1)
```

`lp` returns a fully specified Bayesian network, an object of class `bnc_bn`. 

We can get the CPT of each variable, including the class, with `params`. So, the class prior is 
```{r}
params(b)$class
```

where is the CPT for `buying` is 
```{r}
params(b)$class
```

For more functions that can be called on a `bnc_bn` object see `?bnc_bn_object`

Once we have fit parameters, we can predict the class or class posterior of unseen data (although in this example it is the data we used to learn the model).
```{r}
p <- predict(b, car, prob = TRUE)
head(p)
p <- predict(b, car)
head(p)
```

We can estimate the classifier's predictive accuracy on the training set 
```{r}
accuracy(p, car$class)
```

or with cross-validation.
```{r}

cv(b, car, k = 10, dag = FALSE)
```

# Structure learning 

This section briefly lists the available structure learning algorithms. For additional information see `?bnclassify` and the documentation of each particular function regarding the available options.


## The Chow-Liu algorithm
For some network scores, the Chow-Liu algorithm can efficiently (time quadratic in the number of features) learn optimal one-dependence estimators (i.e., with each feature conditioned on at most one feature). For three such scores, the log-likelihood, the BIC and the AIC, the `tan_cl` function learns the Bayesian network classifier using the Chow-Liu algorithm.

We set the score with the `score` argument.
```{r}
t <- tan_cl(class = 'class', dataset = car)
ta <- tan_cl(class = 'class', dataset = car, score = 'aic')
plot(t)
plot(ta)
```

We can check whether the obtained structures are indeed one-dependence estimators.

```{r}
is_ode(t)
is_nb(t)
is_ode(ta)
is_nb(ta)
```

Note that the BIC and AIC scores may render a forest instead of a tree in the features subgraph. Log-likelihood, on the other hand, always returns the maximal tree-like network. 

See `?tan_chowliu` for more information on the Chow-Liu algorithm for Bayesian network classifiers.

## Wrapper 
Wrapper learners search the space of structures and select the one that optimizes predictive performance. This can yield accurate classifiers but is more time consuming than the Chow-Liu algorithm. Note that this is especially true if the data contains missing values. 

Below are examples of four wrapper learning algorithms. Two of them produce one-dependence estimators (`tan_hc` and `tan_hcsp`) whereas two produce `semi-naive Bayes' structures. See `?wrapper` for more information.

The one-dependence estimators:
```{r}
set.seed(0)
a <- tan_hc('class', car, k = 10, epsilon = 0, smooth = 1)
b <- tan_hcsp('class', car, k = 10, epsilon = 0, smooth = 1)
is_ode(a)
is_ode(b)
plot(a)
```

We can check whether they effectively are one-dependence estimators
```{r}
is_ode(a)
is_ode(b)
```

The semi-naive structure learners:
```{r}
c <- bsej('class', car, k = 10, epsilon = 0, smooth = 1)
d <- fssj('class', car, k = 10, epsilon = 0, smooth = 1)
is_ode(c)
is_ode(d)
is_semi_naive(c)
is_semi_naive(d)
plot(c)
```

# Parameter estimation
You may use the `bnc()` function as shorthand for the chained application of structure learning and `lp()`. Provide the name of the learning function (e.g., `tan_cl`) as first argument.
```{r}
a <- tan_cl('class', car, score = 'aic')
a <- lp(a, car, smooth = 1)
b <- bnc('tan_cl', 'class', car, smooth = 1, dag_args = list(score = 'aic'))
identical(a, b)
```

## Parameter weighting
For naive Bayes, one can combine maximum likelihood and Bayesian parameter estimation with posterior feature parameter weighting. This involves exponentiating the features' CPT entries by a value between 0 and 1 and can alleviate some of the negative effects of redundancy. See `?awnb` for more information.

We use `lpawnb` instead of `lp`.

```{r}
a <- nb('class', car)
b <- lp(a, car, smooth = 1)
c <- lpawnb(a, car, smooth = 1, trees = 20, bootstrap_size = 0.5)
sum(abs(params(b)$safety - params(c)$safety))
```

While this is intented for naive Bayes you can use it with other classifiers.
```{r}
t <- tan_cl('class', car)
t <- lp(t, dataset = car, smooth = 1)
ta <- lpawnb(t, car, smooth = 1, trees = 10, bootstrap_size = 0.5)
params(t)$buying
params(ta)$buying
```

# Predicting 

## 0 probabilities
If for some instance there is 0 probability for each class, then a uniform distribution over the classes is returned (not the class prior). 

```{r}
nb <- nb('class', car)
nb <- lp(nb, car[c(1, 700), ], smooth = 0)
predict(object = nb, newdata = car[1000:1001, ], prob = TRUE)
```

## Incomplete data
For instances that have missing (`NA`) values, `bnclassify` uses the `gRain` package to compute its class posterior, since `gRain` implements exact inference for Bayesian networks. This is much slower than the prediction for complete data implemented in `bnclassify`. 


```{r}
library(microbenchmark)
nb <- nb('class', car)
nb <- lp(nb, car, smooth = 0)
gr <- as_grain(nb)
microbenchmark(predict(object = nb, newdata = car, prob = TRUE))
microbenchmark(gRain::predict.grain(gr, 'class', newdata = car),
                               times = 1)
```

With even a single missing value in the data set, the prediction can become notably slower. This is relevant when performing cross-validation, such as within wrapper learning.

```{r}
a <- bnc('nb', 'class', car, smooth = 1)
car_cv <- car[1:300, ]
microbenchmark::microbenchmark(cv(a, car_cv, k = 2, dag = FALSE), times = 3e1)

car_cv[1, 4] <- NA
microbenchmark::microbenchmark(cv(a, car_cv, k = 2, dag = FALSE), times = 3e1)
```

# Cross-validation
To perform cross valiation, pass a list of classifiers (or a single one) to the 'cv' function. Each classifier may be a `bnc_dag` or a `bnc_bn` object. 

<!-- If the `dag` argument is `TRUE` then structure learning will be performed on each training set. Parameter learning is always performed. -->

In the example below, we compare a naive Bayes, a weighted naive Bayes, and a one-dependence estimators with 3-fold cross-validation. We keep the structures fixed (`dag = FALSE`) and only learn parameters from the training sets.

```{r}
data(voting)
dag <- nb('Class', voting)
a <- lp(dag, voting, smooth = 1)
b <- lpawnb(dag, voting, smooth = 1, trees = 40, bootstrap_size = 0.5)
c <- bnc('tan_cl', 'Class', voting,  smooth = 1)
r <- cv(list(a, b, c), voting, k = 3, dag = FALSE)
r
```

If we wanted to also perform structure learning, we would need to set `dag = TRUE` (this would have only affected the one-dependence estimator, since naive Bayes' structure is fixed).

# Miscelaneous

You can compute the log-likelihood of a network with `compute_ll`.
```{r}
a <- bnc('tan_cl', 'class', car, smooth = 0.01)
b <- bnc('nb', 'class', car, smooth = 0.01)
compute_ll(a, car)
compute_ll(b, car)
```

Also the (conditional) mutual information between two variables. Mutual information of `maint` and `buying`:
```{r}
cmi('maint', 'buying', car)
```
and of `maint` and `buying` conditioned to `class`:
```{r}
cmi('maint', 'buying', car, 'class')
```
# Interface to other packages

You can convert a `bnclassify` object to `bnlearn`, `gRain` and `mlr` objects and use functionalities from those packages. 

## Selecting features with mlr

Some of the implemented algorithms, such as the `fssj` and `bsej` perform implicit feature selection. However, 'outer' loop of feature selection is not within the scope of `bnclassify` and best done with another package such as `mlr`. 

Assuming you have mlr installed, call `as_mlr()` to convert a `bnc_bn` to an mlr *learner*. This allows you to use mlr functionalities: selecting features, benchmarking, etc.

Set up a mlr task 
```{r}
library(mlr)
ct <- mlr::makeClassifTask(id = "compare", data = car, target = 'class', 
                        fixup.data = "no", check.data = FALSE)  
```
Learn a naive Bayes and convert to mlr learner
```{r}
nf <- lp(nb('class', car), car, 1)
bnl <- as_mlr(nf, dag = TRUE)
```
Then use wrapper feature selection
```{r}
ctrl = makeFeatSelControlSequential(alpha = 0, method = "sfs")
rdesc = makeResampleDesc(method = "Holdout")
sfeats = selectFeatures(learner = bnl, task = ct, resampling = rdesc,
                      control = ctrl, show.info = FALSE)
sfeats$x
detach('package:mlr')
```

## Operate with Bayesian networks with `gRain` and `bnlearn`

`gRbase` and `bnlearn` provide multiple functionalities for querying and manipulating Bayesian networks. We can convert a `bnc_bn` to a `gRain` via `as_grain()`. From the `gRain` object you can then obtain a `bnlearn` one (see `bnlearn` docs).

Using `as_grain`:
```{r}
a <- lp(nb('class', car), car, smooth = 1)
g <- as_grain(a)
gRain::querygrain.grain(g)$buying
```

# Runtime 

The wrapper algorithms can be computationlly intensive, especially with large data sets. I get the following times for `bsej` and `tan_hc` on my Windows 2.80 GHz, 16 GB RAM machine. 

```{r, eval = FALSE}
microbenchmark::microbenchmark(
  bsej = {b <- bsej('class', car, k = 10, epsilon = 0)} , 
  tan_hc = {t <- b <- tan_hc('class', car, k = 10, epsilon = 0)}, 
  times = 10)
```
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#> Unit: seconds}
\CommentTok{#>    expr      min       lq     mean   median       uq      max neval}
\CommentTok{#>    bsej 2.578518 2.720906 3.188944 3.341617 3.389287 3.677820    10}
\CommentTok{#>  tan_hc 1.968562 2.201919 2.238606 2.246080 2.361516 2.420327    10}
\end{Highlighting}
\end{Shaded}


10-fold cross-validation of these two classifiers should take rougly 10 times more than learning them the full data set. 


```{r, eval=FALSE}
microbenchmark::microbenchmark(
  cv(list(b, t), car, k = 10, dag = TRUE, smooth = 0.01), times = 10)
```
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#> Unit: seconds}
\CommentTok{#>                                                    expr      min       lq}
\CommentTok{#>  cv(list(b, t), car, k = 10, dag = TRUE, smooth = 0.01) 49.64341 50.63624}
\CommentTok{#>      mean   median       uq      max neval}
\CommentTok{#>  51.28273 51.45354 51.96889 52.28374    10}
\end{Highlighting}
\end{Shaded}

Thus, it takes about a minute to cross-validate these two classifiers on the car data (6 features, 1728 instances). 

Note that non-wrapper classifiers are much faster.

```{r, eval=FALSE}
nb <- nb('class', car)
tcl <- tan_cl('class', car)
microbenchmark::microbenchmark(
  cv(list(nb, tcl), car, k = 10, dag = TRUE, smooth = 0.01), times = 10)
```
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#> Unit: milliseconds}
\CommentTok{#>                                                       expr      min}
\CommentTok{#>  cv(list(nb, tcl), car, k = 10, dag = TRUE, smooth = 0.01) 709.4578}
\CommentTok{#>        lq     mean   median       uq      max neval}
\CommentTok{#>  712.8001 721.6526 720.6743 724.6895 737.8496    10}
\end{Highlighting}
\end{Shaded}
Let us a look at a data set with 36 features. 

```{r, eval=FALSE}
library(mlbench)
data(Soybean)
dim(Soybean)
```
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#> [1] 683  36}
\end{Highlighting}
\end{Shaded}

Inference with incomplete data is slow. Thus, we remove incomplete instances.
```{r, eval=FALSE}
soy_complete <- na.omit(Soybean)
```

`bsej` takes almost 10 minutes. 

```{r, cache = TRUE, eval = FALSE}
microbenchmark::microbenchmark( 
  b <- bsej('Class', soy_complete, k = 10, epsilon = 0), 
  times = 1)
```
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#> Unit: seconds}
\CommentTok{#>                                                   expr      min       lq}
\CommentTok{#>  b <- bsej("Class", soy_complete, k = 10, epsilon = 0) 569.6894 569.6894}
\CommentTok{#>      mean   median       uq      max neval}
\CommentTok{#>  569.6894 569.6894 569.6894 569.6894     1}
\end{Highlighting}
\end{Shaded}

We could expect a 10-fold cross-validation to take around 100 minutes. Note that we have a nested 10 $\times$ 10 cross-validation, though. Decreasing $k$ would decrease runtime and increasing epsilon would likely do the same.